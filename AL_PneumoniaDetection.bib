
@misc{noauthor_pneumonia_nodate,
	title = {Pneumonia {Detection} using {CNN}(92.6\% {Accuracy})},
	url = {https://kaggle.com/code/madz2000/pneumonia-detection-using-cnn-92-6-accuracy},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from Chest X-Ray Images (Pneumonia)},
	language = {en},
	urldate = {2024-12-02},
	file = {Snapshot:/Users/martinusboom/Zotero/storage/T8XZTXK2/notebook.html:text/html},
}

@misc{noauthor_chest_nodate,
	title = {Chest {X}-{Ray} {Images} ({Pneumonia})},
	url = {https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia},
	abstract = {5,863 images, 2 categories},
	language = {en},
	urldate = {2024-12-02},
	file = {Snapshot:/Users/martinusboom/Zotero/storage/JQ4YK7RN/data.html:text/html},
}

@misc{noauthor_activelearner_nodate,
	title = {{ActiveLearner} — {modAL} documentation},
	url = {https://modal-python.readthedocs.io/en/latest/content/models/ActiveLearner.html},
	urldate = {2024-12-02},
	file = {ActiveLearner — modAL documentation:/Users/martinusboom/Zotero/storage/Z68SUE2K/ActiveLearner.html:text/html},
}

@misc{noauthor_uncertainty_nodate,
	title = {Uncertainty sampling — {modAL} documentation},
	url = {https://modal-python.readthedocs.io/en/latest/content/query_strategies/uncertainty_sampling.html},
	urldate = {2024-12-02},
	file = {Uncertainty sampling — modAL documentation:/Users/martinusboom/Zotero/storage/CEZGN5SW/uncertainty_sampling.html:text/html},
}

@misc{noauthor_keras_nodate,
	title = {Keras models in {modAL} workflows — {modAL} documentation},
	url = {https://modal-python.readthedocs.io/en/latest/content/examples/Keras_integration.html},
	urldate = {2024-12-02},
	file = {Keras models in modAL workflows — modAL documentation:/Users/martinusboom/Zotero/storage/N7D7KI2S/Keras_integration.html:text/html},
}

@misc{dey_gentle_2024,
	title = {A {Gentle} {Introduction} to {Active} {Learning}},
	url = {https://arindam-dey.medium.com/a-gentle-introduction-to-active-learning-e983b9d175cb},
	abstract = {I recently ran into a problem of re-training deep learning models. My specific use case had very scarce data and very difficult to gather…},
	language = {en},
	urldate = {2024-12-02},
	journal = {Medium},
	author = {Dey, Arindam},
	month = mar,
	year = {2024},
	file = {Snapshot:/Users/martinusboom/Zotero/storage/FH3FBAGG/a-gentle-introduction-to-active-learning-e983b9d175cb.html:text/html},
}

@misc{noauthor_accuracy_nodate,
	title = {Accuracy vs {Loss} {Conflict} {\textbar} {Kaggle}},
	url = {https://www.kaggle.com/discussions/general/a},
	abstract = {Accuracy vs Loss Conflict.},
	language = {en},
	urldate = {2024-12-02},
	file = {Snapshot:/Users/martinusboom/Zotero/storage/AHB8X6D6/220823.html:text/html},
}

@article{chawla_smote_2002,
	title = {{SMOTE}: {Synthetic} {Minority} {Over}-sampling {Technique}},
	volume = {16},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {{SMOTE}},
	url = {https://www.jair.org/index.php/jair/article/view/10302},
	doi = {10.1613/jair.953},
	abstract = {An approach to the construction of classifiers from    imbalanced datasets is described. A dataset is imbalanced if the    classification categories are not approximately equally    represented. Often real-world data sets are predominately composed of    ``normal'' examples with only a small percentage of ``abnormal'' or    ``interesting'' examples. It is also the case that the cost of    misclassifying an abnormal (interesting) example as a normal example    is often much higher than the cost of the reverse    error. Under-sampling of the majority (normal) class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class. This paper shows that a combination of our method of    over-sampling the minority (abnormal) class and under-sampling the    majority (normal) class can achieve better classifier performance (in    ROC space) than only under-sampling the majority class.  This paper    also shows that a combination of our method of over-sampling the    minority class and under-sampling the majority class can achieve    better classifier performance (in ROC space) than varying the loss    ratios in Ripper or class priors in Naive Bayes. Our method of    over-sampling the minority class involves creating synthetic minority    class examples.  Experiments are performed using C4.5, Ripper and a    Naive Bayes classifier. The method is evaluated using the area under    the Receiver Operating Characteristic curve (AUC) and the ROC convex    hull strategy.},
	language = {en},
	urldate = {2024-12-02},
	journal = {Journal of Artificial Intelligence Research},
	author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
	month = jun,
	year = {2002},
	pages = {321--357},
	file = {Full Text PDF:/Users/martinusboom/Zotero/storage/LSFIZ9R9/Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf:application/pdf},
}

@article{kermany_identifying_2018,
	title = {Identifying {Medical} {Diagnoses} and {Treatable} {Diseases} by {Image}-{Based} {Deep} {Learning}},
	volume = {172},
	issn = {0092-8674, 1097-4172},
	url = {https://www.cell.com/cell/abstract/S0092-8674(18)30154-5},
	doi = {10.1016/j.cell.2018.02.010},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}p{\textgreater}The implementation of clinical-decision support algorithms for medical imaging faces challenges with reliability and interpretability. Here, we establish a diagnostic tool based on a deep-learning framework for the screening of patients with common treatable blinding retinal diseases. Our framework utilizes transfer learning, which trains a neural network with a fraction of the data of conventional approaches. Applying this approach to a dataset of optical coherence tomography images, we demonstrate performance comparable to that of human experts in classifying age-related macular degeneration and diabetic macular edema. We also provide a more transparent and interpretable diagnosis by highlighting the regions recognized by the neural network. We further demonstrate the general applicability of our AI system for diagnosis of pediatric pneumonia using chest X-ray images. This tool may ultimately aid in expediting the diagnosis and referral of these treatable conditions, thereby facilitating earlier treatment, resulting in improved clinical outcomes.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Video Abstract{\textless}/h3{\textgreater}},
	language = {English},
	number = {5},
	urldate = {2024-12-02},
	journal = {Cell},
	author = {Kermany, Daniel S. and Goldbaum, Michael and Cai, Wenjia and Valentim, Carolina C. S. and Liang, Huiying and Baxter, Sally L. and McKeown, Alex and Yang, Ge and Wu, Xiaokang and Yan, Fangbing and Dong, Justin and Prasadha, Made K. and Pei, Jacqueline and Ting, Magdalene Y. L. and Zhu, Jie and Li, Christina and Hewett, Sierra and Dong, Jason and Ziyar, Ian and Shi, Alexander and Zhang, Runze and Zheng, Lianghong and Hou, Rui and Shi, William and Fu, Xin and Duan, Yaou and Huu, Viet A. N. and Wen, Cindy and Zhang, Edward D. and Zhang, Charlotte L. and Li, Oulan and Wang, Xiaobo and Singer, Michael A. and Sun, Xiaodong and Xu, Jie and Tafreshi, Ali and Lewis, M. Anthony and Xia, Huimin and Zhang, Kang},
	month = feb,
	year = {2018},
	pmid = {29474911},
	note = {Publisher: Elsevier},
	pages = {1122--1131.e9},
	file = {Full Text PDF:/Users/martinusboom/Zotero/storage/5VCFG2YP/Kermany et al. - 2018 - Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning.pdf:application/pdf},
}

@misc{zhang_understanding_2017,
	title = {Understanding deep learning requires rethinking generalization},
	url = {http://arxiv.org/abs/1611.03530},
	doi = {10.48550/arXiv.1611.03530},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	urldate = {2024-12-02},
	publisher = {arXiv},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	month = feb,
	year = {2017},
	note = {arXiv:1611.03530},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/martinusboom/Zotero/storage/5TXWN8FM/Zhang et al. - 2017 - Understanding deep learning requires rethinking generalization.pdf:application/pdf;Snapshot:/Users/martinusboom/Zotero/storage/RKLL3FEG/1611.html:text/html},
}

@misc{noauthor_deep_nodate,
	title = {Deep {Learning}},
	url = {https://www.deeplearningbook.org/},
	urldate = {2024-12-02},
	file = {optimization.html:/Users/martinusboom/Zotero/storage/5VVD555E/www.deeplearningbook.org.html:text/html},
}

@misc{sener_active_2018,
	title = {Active {Learning} for {Convolutional} {Neural} {Networks}: {A} {Core}-{Set} {Approach}},
	shorttitle = {Active {Learning} for {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1708.00489},
	doi = {10.48550/arXiv.1708.00489},
	abstract = {Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, ie. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.},
	urldate = {2024-12-02},
	publisher = {arXiv},
	author = {Sener, Ozan and Savarese, Silvio},
	month = jun,
	year = {2018},
	note = {arXiv:1708.00489},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/martinusboom/Zotero/storage/95YG99L6/Sener e Savarese - 2018 - Active Learning for Convolutional Neural Networks A Core-Set Approach.pdf:application/pdf;Snapshot:/Users/martinusboom/Zotero/storage/G85ZIDV9/1708.html:text/html},
}
