<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<title>Bibliografia</title>
</head>
<body>
<div class="csl-bib-body" style="line-height: 1.35; ">
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, ‘SMOTE: Synthetic Minority Over-sampling Technique’, <i>Journal of Artificial Intelligence Research</i>, vol. 16, pp. 321–357, Jun. 2002, doi: <a href="https://doi.org/10.1613/jair.953">10.1613/jair.953</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1613%2Fjair.953&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=SMOTE%3A%20Synthetic%20Minority%20Over-sampling%20Technique&amp;rft.jtitle=Journal%20of%20Artificial%20Intelligence%20Research&amp;rft.volume=16&amp;rft.aufirst=N.%20V.&amp;rft.aulast=Chawla&amp;rft.au=N.%20V.%20Chawla&amp;rft.au=K.%20W.%20Bowyer&amp;rft.au=L.%20O.%20Hall&amp;rft.au=W.%20P.%20Kegelmeyer&amp;rft.date=2002-06-01&amp;rft.pages=321-357&amp;rft.spage=321&amp;rft.epage=357&amp;rft.issn=1076-9757&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[2]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">A. Dey, ‘A Gentle Introduction to Active Learning’, Medium. Accessed: Dec. 02, 2024. [Online]. Available: <a href="https://arindam-dey.medium.com/a-gentle-introduction-to-active-learning-e983b9d175cb">https://arindam-dey.medium.com/a-gentle-introduction-to-active-learning-e983b9d175cb</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=A%20Gentle%20Introduction%20to%20Active%20Learning&amp;rft.description=I%20recently%20ran%20into%20a%20problem%20of%20re-training%20deep%20learning%20models.%20My%20specific%20use%20case%20had%20very%20scarce%20data%20and%20very%20difficult%20to%20gather%E2%80%A6&amp;rft.identifier=https%3A%2F%2Farindam-dey.medium.com%2Fa-gentle-introduction-to-active-learning-e983b9d175cb&amp;rft.aufirst=Arindam&amp;rft.aulast=Dey&amp;rft.au=Arindam%20Dey&amp;rft.date=2024-03-21&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[3]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">D. S. Kermany <i>et al.</i>, ‘Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning’, <i>Cell</i>, vol. 172, no. 5, pp. 1122-1131.e9, Feb. 2018, doi: <a href="https://doi.org/10.1016/j.cell.2018.02.010">10.1016/j.cell.2018.02.010</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2Fj.cell.2018.02.010&amp;rft_id=info%3Apmid%2F29474911&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Identifying%20Medical%20Diagnoses%20and%20Treatable%20Diseases%20by%20Image-Based%20Deep%20Learning&amp;rft.jtitle=Cell&amp;rft.stitle=Cell&amp;rft.volume=172&amp;rft.issue=5&amp;rft.aufirst=Daniel%20S.&amp;rft.aulast=Kermany&amp;rft.au=Daniel%20S.%20Kermany&amp;rft.au=Michael%20Goldbaum&amp;rft.au=Wenjia%20Cai&amp;rft.au=Carolina%20C.%20S.%20Valentim&amp;rft.au=Huiying%20Liang&amp;rft.au=Sally%20L.%20Baxter&amp;rft.au=Alex%20McKeown&amp;rft.au=Ge%20Yang&amp;rft.au=Xiaokang%20Wu&amp;rft.au=Fangbing%20Yan&amp;rft.au=Justin%20Dong&amp;rft.au=Made%20K.%20Prasadha&amp;rft.au=Jacqueline%20Pei&amp;rft.au=Magdalene%20Y.%20L.%20Ting&amp;rft.au=Jie%20Zhu&amp;rft.au=Christina%20Li&amp;rft.au=Sierra%20Hewett&amp;rft.au=Jason%20Dong&amp;rft.au=Ian%20Ziyar&amp;rft.au=Alexander%20Shi&amp;rft.au=Runze%20Zhang&amp;rft.au=Lianghong%20Zheng&amp;rft.au=Rui%20Hou&amp;rft.au=William%20Shi&amp;rft.au=Xin%20Fu&amp;rft.au=Yaou%20Duan&amp;rft.au=Viet%20A.%20N.%20Huu&amp;rft.au=Cindy%20Wen&amp;rft.au=Edward%20D.%20Zhang&amp;rft.au=Charlotte%20L.%20Zhang&amp;rft.au=Oulan%20Li&amp;rft.au=Xiaobo%20Wang&amp;rft.au=Michael%20A.%20Singer&amp;rft.au=Xiaodong%20Sun&amp;rft.au=Jie%20Xu&amp;rft.au=Ali%20Tafreshi&amp;rft.au=M.%20Anthony%20Lewis&amp;rft.au=Huimin%20Xia&amp;rft.au=Kang%20Zhang&amp;rft.date=2018-02-22&amp;rft.pages=1122-1131.e9&amp;rft.spage=1122&amp;rft.epage=1131.e9&amp;rft.issn=0092-8674%2C%201097-4172&amp;rft.language=English"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[4]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">O. Sener and S. Savarese, ‘Active Learning for Convolutional Neural Networks: A Core-Set Approach’, Jun. 01, 2018, <i>arXiv</i>: arXiv:1708.00489. doi: <a href="https://doi.org/10.48550/arXiv.1708.00489">10.48550/arXiv.1708.00489</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1708.00489&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Active%20Learning%20for%20Convolutional%20Neural%20Networks%3A%20A%20Core-Set%20Approach&amp;rft.description=Convolutional%20neural%20networks%20(CNNs)%20have%20been%20successfully%20applied%20to%20many%20recognition%20and%20learning%20tasks%20using%20a%20universal%20recipe%3B%20training%20a%20deep%20model%20on%20a%20very%20large%20dataset%20of%20supervised%20examples.%20However%2C%20this%20approach%20is%20rather%20restrictive%20in%20practice%20since%20collecting%20a%20large%20set%20of%20labeled%20images%20is%20very%20expensive.%20One%20way%20to%20ease%20this%20problem%20is%20coming%20up%20with%20smart%20ways%20for%20choosing%20images%20to%20be%20labelled%20from%20a%20very%20large%20collection%20(ie.%20active%20learning).%20Our%20empirical%20study%20suggests%20that%20many%20of%20the%20active%20learning%20heuristics%20in%20the%20literature%20are%20not%20effective%20when%20applied%20to%20CNNs%20in%20batch%20setting.%20Inspired%20by%20these%20limitations%2C%20we%20define%20the%20problem%20of%20active%20learning%20as%20core-set%20selection%2C%20ie.%20choosing%20set%20of%20points%20such%20that%20a%20model%20learned%20over%20the%20selected%20subset%20is%20competitive%20for%20the%20remaining%20data%20points.%20We%20further%20present%20a%20theoretical%20result%20characterizing%20the%20performance%20of%20any%20selected%20subset%20using%20the%20geometry%20of%20the%20datapoints.%20As%20an%20active%20learning%20algorithm%2C%20we%20choose%20the%20subset%20which%20is%20expected%20to%20yield%20best%20result%20according%20to%20our%20characterization.%20Our%20experiments%20show%20that%20the%20proposed%20method%20significantly%20outperforms%20existing%20approaches%20in%20image%20classification%20experiments%20by%20a%20large%20margin.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1708.00489&amp;rft.aufirst=Ozan&amp;rft.aulast=Sener&amp;rft.au=Ozan%20Sener&amp;rft.au=Silvio%20Savarese&amp;rft.date=2018-06-01"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[5]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, ‘Understanding deep learning requires rethinking generalization’, Feb. 26, 2017, <i>arXiv</i>: arXiv:1611.03530. doi: <a href="https://doi.org/10.48550/arXiv.1611.03530">10.48550/arXiv.1611.03530</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1611.03530&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Understanding%20deep%20learning%20requires%20rethinking%20generalization&amp;rft.description=Despite%20their%20massive%20size%2C%20successful%20deep%20artificial%20neural%20networks%20can%20exhibit%20a%20remarkably%20small%20difference%20between%20training%20and%20test%20performance.%20Conventional%20wisdom%20attributes%20small%20generalization%20error%20either%20to%20properties%20of%20the%20model%20family%2C%20or%20to%20the%20regularization%20techniques%20used%20during%20training.%20Through%20extensive%20systematic%20experiments%2C%20we%20show%20how%20these%20traditional%20approaches%20fail%20to%20explain%20why%20large%20neural%20networks%20generalize%20well%20in%20practice.%20Specifically%2C%20our%20experiments%20establish%20that%20state-of-the-art%20convolutional%20networks%20for%20image%20classification%20trained%20with%20stochastic%20gradient%20methods%20easily%20fit%20a%20random%20labeling%20of%20the%20training%20data.%20This%20phenomenon%20is%20qualitatively%20unaffected%20by%20explicit%20regularization%2C%20and%20occurs%20even%20if%20we%20replace%20the%20true%20images%20by%20completely%20unstructured%20random%20noise.%20We%20corroborate%20these%20experimental%20findings%20with%20a%20theoretical%20construction%20showing%20that%20simple%20depth%20two%20neural%20networks%20already%20have%20perfect%20finite%20sample%20expressivity%20as%20soon%20as%20the%20number%20of%20parameters%20exceeds%20the%20number%20of%20data%20points%20as%20it%20usually%20does%20in%20practice.%20We%20interpret%20our%20experimental%20findings%20by%20comparison%20with%20traditional%20models.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1611.03530&amp;rft.aufirst=Chiyuan&amp;rft.aulast=Zhang&amp;rft.au=Chiyuan%20Zhang&amp;rft.au=Samy%20Bengio&amp;rft.au=Moritz%20Hardt&amp;rft.au=Benjamin%20Recht&amp;rft.au=Oriol%20Vinyals&amp;rft.date=2017-02-26"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[6]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">‘Accuracy vs Loss Conflict | Kaggle’. Accessed: Dec. 02, 2024. [Online]. Available: <a href="https://www.kaggle.com/discussions/general/a">https://www.kaggle.com/discussions/general/a</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Accuracy%20vs%20Loss%20Conflict%20%7C%20Kaggle&amp;rft.description=Accuracy%20vs%20Loss%20Conflict.&amp;rft.identifier=https%3A%2F%2Fwww.kaggle.com%2Fdiscussions%2Fgeneral%2Fa&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[7]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">‘ActiveLearner — modAL documentation’. Accessed: Dec. 02, 2024. [Online]. Available: <a href="https://modal-python.readthedocs.io/en/latest/content/models/ActiveLearner.html">https://modal-python.readthedocs.io/en/latest/content/models/ActiveLearner.html</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=ActiveLearner%20%E2%80%94%20modAL%20documentation&amp;rft.identifier=https%3A%2F%2Fmodal-python.readthedocs.io%2Fen%2Flatest%2Fcontent%2Fmodels%2FActiveLearner.html"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[8]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">‘Chest X-Ray Images (Pneumonia)’. Accessed: Dec. 02, 2024. [Online]. Available: <a href="https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia">https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Chest%20X-Ray%20Images%20(Pneumonia)&amp;rft.description=5%2C863%20images%2C%202%20categories&amp;rft.identifier=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Fpaultimothymooney%2Fchest-xray-pneumonia&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[9]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">‘Deep Learning’. Accessed: Dec. 02, 2024. [Online]. Available: <a href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Deep%20Learning&amp;rft.identifier=https%3A%2F%2Fwww.deeplearningbook.org%2F"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[10]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">‘Keras models in modAL workflows — modAL documentation’. Accessed: Dec. 02, 2024. [Online]. Available: <a href="https://modal-python.readthedocs.io/en/latest/content/examples/Keras_integration.html">https://modal-python.readthedocs.io/en/latest/content/examples/Keras_integration.html</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Keras%20models%20in%20modAL%20workflows%20%E2%80%94%20modAL%20documentation&amp;rft.identifier=https%3A%2F%2Fmodal-python.readthedocs.io%2Fen%2Flatest%2Fcontent%2Fexamples%2FKeras_integration.html"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[11]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">‘Pneumonia Detection using CNN(92.6% Accuracy)’. Accessed: Dec. 02, 2024. [Online]. Available: <a href="https://kaggle.com/code/madz2000/pneumonia-detection-using-cnn-92-6-accuracy">https://kaggle.com/code/madz2000/pneumonia-detection-using-cnn-92-6-accuracy</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Pneumonia%20Detection%20using%20CNN(92.6%25%20Accuracy)&amp;rft.description=Explore%20and%20run%20machine%20learning%20code%20with%20Kaggle%20Notebooks%20%7C%20Using%20data%20from%20Chest%20X-Ray%20Images%20(Pneumonia)&amp;rft.identifier=https%3A%2F%2Fkaggle.com%2Fcode%2Fmadz2000%2Fpneumonia-detection-using-cnn-92-6-accuracy&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[12]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">‘Uncertainty sampling — modAL documentation’. Accessed: Dec. 02, 2024. [Online]. Available: <a href="https://modal-python.readthedocs.io/en/latest/content/query_strategies/uncertainty_sampling.html">https://modal-python.readthedocs.io/en/latest/content/query_strategies/uncertainty_sampling.html</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Uncertainty%20sampling%20%E2%80%94%20modAL%20documentation&amp;rft.identifier=https%3A%2F%2Fmodal-python.readthedocs.io%2Fen%2Flatest%2Fcontent%2Fquery_strategies%2Funcertainty_sampling.html"></span>
</div></body>
</html>
